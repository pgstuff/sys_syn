:toc:
:toclevels: 4



= sys_syn



== Description

An asynchronous, loosely coupled, one-to-many system replication kit.



== Synopsis

`sys_syn` synchronizes objects from one system to one or more systems.  As systems have different DDL for similar objects, `sys_syn` provides transformations at various stages.  There are simple built-in rules that normalize data by database server software data type limitations.  This happens before and after synchronization to match the source and destination server's data type limitations, respectively.  Custom rules may be added and applied to handle similar conventions in the source and destination application systems.  More complex transformation is performed by procedures that read the changeset queue table.

`sys_syn` is intended to operate between different database software.  The source database may be accessed via foreign data wrappers (FDWs).  The destination database can query the queue table via a view that provides compatibility with non-PostgreSQL database server software.

The workflow is:

. An optional prepull operation fetches data from a specified group of tables before the next steps are run.
. Pull or push (full tables or a subset of whole objects).  The pull runs the input transformations.
. Create a changeset based on the last successfully processed objects.
.. If this is a full dataset, there is an option to add objects that are missing from the full dataset and were processed.  This is used to indicate the removal of objects.
. The queue processor claims the changeset that it is currently available to it.
. The queue processor accesses the claimed records via an optional view.  The view contains output transformations.
. The queue processor updates the queue with a per object success or fail status.
. The baseline is updated with the successful records so that future changesets can be generated.

TIP:  If there are multiple destination systems, a single pull or push operation can supply data to multiple destination systems.  Each destination system will have its own changeset.  The `attributes` and `no_diff` columns are stored in a versioned record that is shared between multiple outputs for storage optimization and performance.

If foreign keys are specified, then the changeset omits objects whose parent objects have never been successfully processed by the destination system.

If a new object update cannot be published in the queue because a processor has claimed the object, or an object has been withheld from the queue because all of its parent objects have not been processed, the `move` function will add the object to the queue when these conditions have cleared without the need for a subsequent pull operation.

The queue table allows for concurrent batching and robust error workflows.

This workflow is run by executing generated stored procedures.  There are functions to generate lines for crontab or inserting jobs into pgAgent if you want to run these procedures automatically via either of those technologies.  You may also push rows and call the procedures manually for an event driven workflow.



== User Guide



=== Requirements

Usage requirements:

- PostgreSQL 9.5 or above.
- The `pgcrypto` PostgreSQL extension.

Test requirements:

- The `tinyint` PostgreSQL extension.

Documentation requirements:

- `asciidoc`
- `source-highlight`



=== Installation



==== Per Server Installation

[source,shell]
----
sudo PATH=$PATH make clean && sudo PATH=$PATH make install && make installcheck
----



==== Per Database Installation

You only need to run this on the database(s) that will run `sys_syn`.

[source,sql]
----
CREATE EXTENSION pgcrypto;
CREATE EXTENSION sys_syn;
----

If you use the sys_syn_dblink extension, be aware that it will store groups and foreign keys that are specific to this database (or a cluster of databases if you are using logical replication).  Because sys_syn_dblink supports multiple sys_syn clusters, it uses a unique identifier called cluster_id to associate these groups and keys to the correct sys_syn cluster.  By default, it is set to a GUID.   You may change it.

The settings table is empty until sys_syn is used to move data.  You can insert a settings record before then.

[source,sql]
----
INSERT INTO sys_syn.settings(cluster_id) VALUES ('sys_syn-test');
----

If a settings record already exists, you may change the cluster_id with:

[source,sql]
----
UPDATE sys_syn.settings SET cluster_id = 'sys_syn-test';
----

CAUTION:  Do not change the cluster_id if a sys_syn_dblink database is referencing it.



=== Usage



==== Setup



===== Example Schema & Data

The following examples assume the following schema and data:

[source,sql]
----
CREATE SCHEMA user_data
    AUTHORIZATION postgres;

CREATE TABLE user_data.test_table (
        test_table_id integer NOT NULL,
        test_table_text text,
        CONSTRAINT test_table_pkey PRIMARY KEY (test_table_id));

INSERT INTO user_data.test_table(
        test_table_id, test_table_text)
VALUES (1,              'test_data1');

INSERT INTO user_data.test_table(
        test_table_id, test_table_text)
VALUES (2,              'test_data2');
----



===== Add an Input Group

An input group identifies the source system or application.  You may associate custom transformation rules to an input group.  You can have a hierarchy of input groups if you want multiple levels of transformation rules.  Specify the parent's `in_group_id` in the `parent_in_group_id` column of a child input group.  The child input group will inherent the rules of its ancestors.

[source,sql]
----
INSERT INTO sys_syn.in_groups_def VALUES ('in');
----



===== Add an Input Table

You may add an input table immediately using:

[source,sql]
----
DO $$BEGIN
        EXECUTE sys_syn.in_table_create_sql('user_data.test_table'::regclass, 'in');
END$$;
----

IMPORTANT:  If the table is a foreign data wrapper (FDW), then you must specify the primary key or ID by adding "`, id_columns => ARRAY['id_col_name_here']`" to the `sys_syn.in_table_create_sql` function call.

TIP:  If an object is composed of multiple rows because the rows represent versions of the same object, then leave the timestamp field off of the Id and mark the timestamp as an Attribute with an array_order of 1.  This groups the object's rows into a single queue record and allows you to process the object change with the complete history.  Enable the data_view to see the versions as distinct rows with a single queue record.  This ensures that the object is either committed as a whole, or not at all.

You can also generate the function call to add the table by specifying just the table and `in_group`.

[source,sql]
----
SELECT sys_syn.in_table_create_sql('user_data.test_table'::regclass, 'in');
----

Copy the resulting text into your SQL editor, make adjustments, and execute it.

[source,sql]
----
SELECT  sys_syn.in_table_create(
                schema          => 'user_data'::regnamespace,
                in_table_id     => 'test_table',
                in_group_id     => 'in',
                in_pull_id      => NULL,
                in_columns     => ARRAY[
                       $COL$("test_table_id","integer",Id,"in_source.test_table_id",,,,,)$COL$,
                       $COL$("test_table_text","text",Attribute,"in_source.test_table_text",,,,,)$COL$
                ]::sys_syn.create_in_column[],
                full_table_reference    => 'user_data.test_table',
                changes_table_reference => NULL,
                full_sql                => NULL,
                changes_sql             => NULL,
                full_pre_sql            => NULL,
                changes_pre_sql         => NULL,
                full_post_sql           => NULL,
                changes_post_sql        => NULL,
                enable_deletes_implied  => TRUE,
                full_prepull_id         => NULL,
                changes_prepull_id      => NULL,
                in_partitions           => ARRAY[
                        $PART$("",)$PART$]::sys_syn.create_in_partition[]
        );
----



===== Add New Columns

[source,sql]
----
SELECT sys_syn.in_table_columns_add_sql('test_table');
----

Copy the resulting text into your SQL editor, make adjustments, and execute it.

You may also add the new columns immediately using:

[source,sql]
----
DO $$BEGIN
        EXECUTE sys_syn.in_table_columns_add_sql('test_table');
END$$;
----



===== Drop an Input Table

Change the boolean to true to drop all associated output tables.

[source,sql]
----
SELECT sys_syn.in_table_drop('test_table', false);
----

If you want to drop the pull as well, run:

[source,sql]
----
SELECT sys_syn.in_pull_drop('test_table');
----



===== Add an Output Group

An output group identifies the destination system or application.  You may associate custom transformation rules to an output group.  You can have a hierarchy of output groups if you want multiple levels of transformation rules.  Specify the parent's `out_group_id` in the `parent_out_group_id` column of a child output group.  The child output group will inherent the rules of its ancestors.

[source,sql]
----
INSERT INTO sys_syn.out_groups_def VALUES ('out');
----



===== Add an Output Table

You may add an output table immediately using:

[source,sql]
----
SELECT sys_syn.out_table_create('user_data', 'test_table', 'out', data_view => false);
----

The arguments are:

. Schema name
. Table name
. Out group ID
. Create a data view

If you want to change the advanced parameters or manually review or edit the transformations, run:

[source,sql]
----
SELECT sys_syn.out_table_create_sql('user_data', 'test_table', 'out', data_view => false);
----

Copy the resulting text into your SQL editor, make adjustments, and execute it.

Setting data_view to true will create a view that will put the data columns into a single record, instead of requiring a join to the _in table.  If you add columns later, the view will have to be recreated before you can use them.  This extra step can be desirable if you want to maintain a stable API via the view while adding columns for the other outputs.  You can update the queue status columns via the view.

[source,sql]
----
SELECT  sys_syn.out_table_create (
                schema                  => 'user_data'::regnamespace,
                in_table_id             => 'test_table',
                out_group_id            => 'out',
                out_columns             => ARRAY[
                       $COL$("sys_syn_trans_id_in","out_queue.trans_id_in",,,)$COL$,
                       $COL$("sys_syn_delta_type","out_queue.delta_type",,,)$COL$,
                       $COL$("sys_syn_queue_state","out_queue.queue_state",queue_state,"new.sys_syn_queue_state",)$COL$,
                       $COL$("sys_syn_queue_id","out_queue.queue_id",queue_id,"new.sys_syn_queue_id",)$COL$,
                       $COL$("sys_syn_queue_priority","out_queue.queue_priority",queue_priority,"new.sys_syn_queue_priority",)$COL$,
                       $COL$("sys_syn_hold_updated","out_queue.hold_updated",,,)$COL$,
                       $COL$("sys_syn_hold_trans_id_first","out_queue.hold_trans_id_first",,,)$COL$,
                       $COL$("sys_syn_hold_trans_id_last","out_queue.hold_trans_id_last",,,)$COL$,
                       $COL$("sys_syn_hold_reason_count","out_queue.hold_reason_count",,,)$COL$,
                       $COL$("sys_syn_hold_reason_id","out_queue.hold_reason_id",hold_reason_id,"new.sys_syn_hold_reason_id",)$COL$,
                       $COL$("sys_syn_hold_reason_text","out_queue.hold_reason_text",hold_reason_text,"new.sys_syn_hold_reason_text",)$COL$,
                       $COL$("sys_syn_trans_id_out","out_queue.trans_id_out",,,)$COL$,
                       $COL$("sys_syn_processed_time","out_queue.processed_time",processed_time,"new.sys_syn_processed_time",)$COL$,
                       $COL$("test_table_id","(out_queue.id).test_table_id",,,Id)$COL$,
                       $COL$("test_table_text","(in_source.attributes).test_table_text",,,Attribute)$COL$
                ]::sys_syn.create_out_column[],
                data_view               => 'false',
                out_log_lifetime        => NULL,
                out_partitions          => ARRAY[
                       $PART$()$PART$]::sys_syn.create_out_partition[],
                enable_adds             => 'true',
                enable_changes          => 'true',
                enable_deletes          => 'true',
                condition_sql           => NULL,
                claim_limit_rows        => '2147483647',
                claim_queue_count       => NULL,
                claim_fixed_by_id       => 'false',
                claim_random_sample     => NULL,
                queue_pid_used_age      => NULL,
                record_comparison_different=> NULL,
                record_comparison_same     => NULL
        );
);
----

===== Drop an Output Table

[source,sql]
----
SELECT sys_syn.out_table_drop('test_table', 'out');
----



==== Runtime Implementation



===== Pull Data

Pull the data from the source system using:

[source,sql]
----
SELECT user_data.test_table_pull(FALSE);
----

A boolean is returned.  False indicates that there are no records to process and that the following steps do not need to be run at this time.  True indicates that the following steps are ready to run.



===== Refresh the Output Queue

Refresh the changeset queue by calling the output group's move function:

[source,sql]
----
SELECT user_data.test_table_out_move_1();
----

A boolean is returned.  False indicates that there are no records to process and that the following steps do not need to be run at this time.  True indicates that the following steps are ready to run.

IMPORTANT:  The `move` function must be run in a transaction that is separate from the `pull` and `processed` functions.



===== Process the Output Queue

First, claim the `Unclaimed` records in the queue for processing by setting the `queue_state` to the `Claimed` status.

[source,sql]
----
SELECT  sys_syn.in_trans_claim_start();

UPDATE  user_data.test_table_out_queue_1
SET     queue_state = 'Claimed'::sys_syn.queue_state
WHERE   queue_state = 'Unclaimed'::sys_syn.queue_state;

SELECT  sys_syn.in_trans_finish();
----

Next, read only the records that have the `Claimed` status.

[source,sql]
----
SELECT  out_queue.*,
        (in_data.id).*,
        (in_data.attributes).*
FROM    user_data.test_table_out_queue_1 AS out_queue
        LEFT JOIN user_data.test_table_in_1 AS in_data USING (trans_id_in, id)
WHERE   queue_state = 'Claimed'::sys_syn.queue_state;
----

Process the records in your destination system.  For records that were processed successfully, set their `queue_state` to `Processed`.

If records failed to process, set their status to `Hold` or `Unclaimed`.  The `Hold` status allows you to process failed records at less frequent intervals.  The `Hold` status requires that you set `hold_reason_id` and/or `hold_reason_text`.

TIP:  If you update the `sys_syn` columns via the data_view, then you need to add `sys_syn_` in front of each `sys_syn` column's name.

[source,sql]
----
UPDATE  user_data.test_table_out_queue_1 AS out_queue
SET     queue_state = 'Processed'::sys_syn.queue_state
WHERE   (out_queue.id).test_table_id = 1;

UPDATE  user_data.test_table_out_queue_1 AS out_queue
SET     queue_state = 'Hold'::sys_syn.queue_state,
        hold_reason_text = 'This object has been put on hold for an example.'
WHERE   (out_queue.id).test_table_id = 2;
----



===== Commit the Processing Statuses

Updating the `queue_state` does not automatically commit the processing status.  Call the output's `processed` function to commit the processed changes.  This removes processed records from the queue table and commits them into baseline status so that future changesets only contain actual changes.

[source,sql]
----
SELECT user_data.test_table_out_processed_1();
----

A boolean is returned.  False indicates that there was nothing to do.  True indicates that the queue state was changed.



===== Error Handling

If you use the `Hold` status, then you must set the `Hold` status back to `Unclaimed` when you want to retry those records.  The `hold_reason_count` value is incremented if the error is the same error that was recorded in the prior processing attempt.  This allows you to implement a backoff algorithm to avoid wasting resources on a potentially non-transient failure.

If the object changes value while in the `Hold` status, then its queue status is automatically reset to `Unclaimed`.  This allows data corrections to be retried without a `Hold` delay.



=== Testing Performance Enhancement

Optionally, you can initialize a database server in shared memory to avoid disk I/O.  This useful if you need to run the tests frequently.



==== Initialize In Memory Server

[source,shell]
----
export PGDATA=/dev/shm/$USER-pg_regression_test
mkdir "$PGDATA"
initdb --auth-local=peer --auth-host=ident -U postgres -N "$PGDATA"
cat << "EOF" >> "$PGDATA/postgresql.conf"
fsync = off
synchronous_commit = off
full_page_writes = off
random_page_cost = 1.0
update_process_title = off
EOF
echo "CREATE ROLE $USER SUPERUSER CREATEDB CREATEROLE INHERIT LOGIN" | postmaster --single -D "$PGDATA" -F -h "" -k "$PGDATA" postgres && echo
postmaster -D "$PGDATA" -F -h "" -k "$PGDATA" & sleep 2; echo
export PGHOST=$PGDATA
----

CAUTION:  Every program launched in this terminal will point to this in memory instance.  Be careful not to accidentally put non-ephemeral data or code there.

CAUTION:  Remember that everything created in this database will disappear after a reboot, shutdown, or machine crash.

TIP:  To view this instance in pgAdmin3, set the +Host+ to +/dev/shm/$USER-pg_regression_test+, replace +$USER+ with your user name (run +echo $USER+ if you do not know what it is), and leave the +Port+ number as +5432+.  Use the same user name for the +Username+ field.  When prompted for a password, leave it blank or enter any non-blank value to save it.



==== Run Tests

[source,shell]
----
sudo PATH=$PATH make clean && sudo PATH=$PATH make install && make installcheck
----



==== Shutdown & Delete In Memory Server

The following commands will shutdown the server and permanently delete all of the data that was created within that server.

[source,shell]
----
fg 1
----

Hold Ctrl and press C.

[source,shell]
----
rm -Rf "/dev/shm/$USER-pg_regression_test"
unset PGDATA
unset PGHOST
----



=== PL/pgSQL Debugger

You can use the PL/pgSQL debugger in pgAdmin3 if you build and install the following extension.  You may want to change the install directory and use a different server restart command depending on your distribution and instance.  If you have access, the `/usr/local/src` directory is a good location to store the source code.  However, you will not need it again.  If you upgrade PostgreSQL to a different major version, you will need to download a fresh copy and install it again.

CAUTION:  If you already have something in `shared_preload_libraries`, then manually edit `$PGDATA/postgresql.conf` and add `$libdir/plugin_debugger` to `shared_preload_libraries` instead of running the `cat` command below.

[source,shell]
----
cd /dev/shm
curl -LO "http://ftp.postgresql.org/pub/source/v$(pg_config --version | cut -f 2 -d ' ')/postgresql-$(pg_config --version | cut -f 2 -d ' ').tar.bz2"
tar -xjf postgresql-$(pg_config --version | cut -f 2 -d ' ').tar.bz2
cd postgresql-$(pg_config --version | cut -f 2 -d ' ')
USE_PGXS=1 ./configure
USE_PGXS=1 make
cd contrib
git clone "git://git.postgresql.org/git/pldebugger.git"
make
cd pldebugger
USE_PGXS=1 make
sudo USE_PGXS=1 PATH=$PATH make install

cat << "EOF" >> "$PGDATA/postgresql.conf"
shared_preload_libraries = '$libdir/plugin_debugger'
EOF

pg_ctl restart
----

After the restart, you need to add the `pldbgapi` extension on each database that you want to use the debugger with.

[source,sql]
----
CREATE EXTENSION pldbgapi;
----



=== More Examples

See the `test` directory for more examples.



== Copyright and License

Copyright (c) 2016-2017.

Legal Notice:  See the COPYRIGHT file.

`sys_syn` copyright is novated to PostgreSQL Global Development Group.
