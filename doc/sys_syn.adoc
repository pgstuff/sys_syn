:toc:
:toclevels: 4



= sys_syn



== Description

An asynchronous, loosely coupled, one-to-many system replication kit.



== Synopsis

`sys_syn` synchronizes objects from one system to one or more systems.  As systems have different DDL for similar objects, `sys_syn` provides transformations at various stages.  There are simple built-in rules that normalize data by database server software data type limitations.  This happens before and after synchronization to match the source and destination server's data type limitations, respectively.  Custom rules may be added and applied to handle similar conventions in the source and destination application systems.  More complex transformation is performed by procedures that read the changeset queue table.

`sys_syn` is intended to operate between different database software.  The source database may be accessed via foreign data wrappers (FDWs).  The destination database can query the queue table via a view that provides compatibility with non-PostgreSQL database server software.

The runtime workflow is:

. An optional prepull operation fetches data from a specified group of tables before the next steps are run.
. Pull or push (full tables or a subset of whole objects).  The pull runs the input transformations.
. Create a changeset based on the last successfully processed objects.
.. If this is a full dataset, there is an option to add objects that are missing from the full dataset and were processed.  This is used to indicate the removal of objects.
. The queue processor claims the changeset that it is currently available to it.
. The queue processor accesses the claimed records via an optional view.  The view contains output transformations.
. The queue processor updates the queue with a per object success or fail status.
. The baseline is updated with the successful records so that future changesets can be generated.

TIP:  If there are multiple destination systems, a single pull or push operation can supply data to multiple destination systems.  Each destination system will have its own changeset.  The `attributes` and `no_diff` columns are stored in a versioned record that is shared between multiple outputs for storage optimization and performance.

If foreign keys are specified, then the changeset omits objects whose parent objects have never been successfully processed by the destination system.

If a new object update cannot be published in the queue because a processor has claimed the object, or an object has been withheld from the queue because all of its parent objects have not been processed, the `move` function will add the object to the queue when these conditions have cleared without the need for a subsequent pull operation.

The queue table allows for concurrent batching and robust error workflows.

This workflow is run by executing generated stored procedures.  There are functions to generate lines for crontab or inserting jobs into pgAgent if you want to run these procedures automatically via either of those technologies.  You may also push rows and call the procedures manually for an event driven workflow.

The setup workflow is:

* Install the `sys_syn` extension.

* Optionally set a `cluster_id`.

* Create 1 or more in groups (`sys_syn.in_groups_def`).

* Optionally add in column transform rules to `sys_syn.in_column_transforms`.  This is only used for `in_table_create_sql`.

* Optionally add foreign keys to `sys_syn.foreign_keys_for_c_sql`.  This is only used for `in_table_create_sql`.

* Optionally use `sys_syn.in_table_create_sql` to create the `sys_syn.in_table_create` call.

* Create 1 or more input tables (`sys_syn.in_table_create`).

* Create 1 or more out groups (`sys_syn.out_groups_def`).

* Optionally use `sys_syn.out_table_create_sql` to create the `sys_syn.out_table_create` call.

* Create 1 or more output tables (`sys_syn.out_table_create`).



== User Guide



=== Requirements

Usage requirements:

- PostgreSQL 9.5 or above.

Test requirements:

- The `tinyint` PostgreSQL extension.

Documentation requirements:

- `asciidoc`
- `source-highlight`



=== Installation



==== Per Server Installation

[source,shell]
----
sudo PATH=$PATH make clean && sudo PATH=$PATH make install && make installcheck
----



==== Per Database Installation

You only need to run this on the database(s) that will run `sys_syn`.

[source,sql]
----
CREATE EXTENSION sys_syn;
----

If you use the `sys_syn_dblink` extension, be aware that it will store groups and foreign keys that are specific to this database (or a cluster of databases if you are using logical replication).  Because `sys_syn_dblink` supports multiple `sys_syn` clusters, it uses a unique identifier called `cluster_id` to associate these groups and keys to the correct `sys_syn` cluster.  By default, it is set to a GUID.   You may change it.

The settings table is empty until `sys_syn` is used to move data.  You can insert a settings record before then.

[source,sql]
----
INSERT INTO sys_syn.settings(cluster_id) VALUES ('sys_syn-test');
----

If a settings record already exists, you may change the cluster_id with:

[source,sql]
----
UPDATE sys_syn.settings SET cluster_id = 'sys_syn-test';
----

WARNING:  Do not change the `cluster_id` if a `sys_syn_dblink` database is referencing it.



=== Usage



==== Setup



===== Example Schema & Data

The following examples assume the following schema and data:

[source,sql]
----
CREATE SCHEMA user_data
    AUTHORIZATION postgres;

CREATE TABLE user_data.test_table (
        test_table_id integer NOT NULL,
        test_table_text text,
        CONSTRAINT test_table_pkey PRIMARY KEY (test_table_id));

INSERT INTO user_data.test_table(
        test_table_id, test_table_text)
VALUES (1,              'test_data1');

INSERT INTO user_data.test_table(
        test_table_id, test_table_text)
VALUES (2,              'test_data2');
----



===== Add an Input Group

An input group identifies the source system or application.  You may associate custom transformation rules to an input group.  You can have a hierarchy of input groups if you want multiple levels of transformation rules.  Specify the parent's `in_group_id` in the `parent_in_group_id` column of a child input group.  The child input group will inherent the rules of its ancestors.

[source,sql]
----
INSERT INTO sys_syn.in_groups_def VALUES ('in');
----

CAUTION:  If the source database technology ignores trailing spaces for equality operations, then you should use an `in_column_transform` to `rtrim` key columns on both primary and foreign keys.  There are some stock `rule_group_id`'s that set this up for you, but you have to declare foreign keys for joins to be reproduced accurately.

In this example, the `sys_syn-general` `rule_group_id` and one other `rule_group_id` (to be altered to match your source database technology) are activated to setup the proper string trimming when foreign keys are declared.

[source,sql]
----
INSERT INTO sys_syn.in_groups_def
        (in_group_id,   parent_in_group_id,     rule_group_ids)
VALUES  ('in',          NULL,                   ARRAY['sys_syn-CHANGETHIS','sys_syn-general']);
----



===== Add an Input Table

You may add an input table immediately using:

[source,sql]
----
DO $$BEGIN
        EXECUTE sys_syn.in_table_create_sql('user_data.test_table'::regclass, 'in');
END$$;
----

IMPORTANT:  If the table is a foreign data wrapper (FDW), then you must specify the primary key or ID by adding "`, id_columns => ARRAY['id_col_name_here']`" to the `sys_syn.in_table_create_sql` function call.

TIP:  If an object is composed of multiple rows because the rows represent versions of the same object, then leave the timestamp field off of the Id and mark the timestamp as an Attribute with an array_order of 1.  This groups the object's rows into a single queue record and allows you to process the object change with the complete history.  Enable the data_view to see the versions as distinct rows with a single queue record.  This ensures that the object is either committed as a whole, or not at all.

You can also generate the function call to add the table by specifying just the table and `in_group`.

[source,sql]
----
SELECT sys_syn.in_table_create_sql('user_data.test_table'::regclass, 'in');
----

Copy the resulting text into your SQL editor, make adjustments, and execute it.

[source,sql]
----
SELECT  sys_syn.in_table_create(
                schema          => 'user_data'::regnamespace,
                in_table_id     => 'test_table',
                in_group_id     => 'in',
                in_pull_id      => NULL,
                in_columns      => ARRAY[
                       $COL$("test_table_id","integer",Id,"in_source.test_table_id",,,,,)$COL$,
                       $COL$("test_table_text","text",Attribute,"in_source.test_table_text",,,,,)$COL$
                ]::sys_syn.create_in_column[],
                full_table_reference    => 'user_data.test_table',
                changes_table_reference => NULL,
                full_sql                => NULL,
                changes_sql             => NULL,
                full_pre_sql            => NULL,
                changes_pre_sql         => NULL,
                full_post_sql           => NULL,
                changes_post_sql        => NULL,
                enable_deletes_implied  => 'true',
                null_key_handler        => 'none'::sys_syn.null_key_handler,
                key_violation_handler   => 'none'::sys_syn.key_violation_handler,
                full_prepull_id         => NULL,
                changes_prepull_id      => NULL,
                record_comparison_different=>NULL,
                record_comparison_same  => NULL,
                in_partitions           => ARRAY[
                        $PART$("",)$PART$]::sys_syn.create_in_partition[]
        );
----



===== Add New Columns

[source,sql]
----
SELECT sys_syn.in_table_columns_add_sql('test_table');
----

Copy the resulting text into your SQL editor, make adjustments, and execute it.

You may also add the new columns immediately using:

[source,sql]
----
DO $$BEGIN
        EXECUTE sys_syn.in_table_columns_add_sql('test_table');
END$$;
----



===== Drop an Input Table

Change the boolean to true to drop all associated output tables.

[source,sql]
----
SELECT sys_syn.in_table_drop('test_table', false);
----

If you want to drop the pull as well, run:

[source,sql]
----
SELECT sys_syn.in_pull_drop('test_table');
----



===== Add an Output Group

An output group identifies the destination system or application.  You may associate custom transformation rules to an output group.  You can have a hierarchy of output groups if you want multiple levels of transformation rules.  Specify the parent's `out_group_id` in the `parent_out_group_id` column of a child output group.  The child output group will inherent the rules of its ancestors.

[source,sql]
----
INSERT INTO sys_syn.out_groups_def VALUES ('out');
----



===== Add an Output Table

You may add an output table immediately using:

[source,sql]
----
SELECT sys_syn.out_table_create('user_data', 'test_table', 'out', data_view => false);
----

The arguments are:

. Schema name
. Table name
. Out group ID
. Create a data view

If you want to change the advanced parameters or manually review or edit the transformations, run:

[source,sql]
----
SELECT sys_syn.out_table_create_sql('user_data', 'test_table', 'out', data_view => false);
----

Copy the resulting text into your SQL editor, make adjustments, and execute it.

Setting data_view to true will create a view that will put the data columns into a single record, instead of requiring a join to the _in table.  If you add columns later, the view will have to be recreated before you can use them.  This extra step can be desirable if you want to maintain a stable API via the view while adding columns for the other outputs.  You can update the queue status columns via the view.

[source,sql]
----
SELECT  sys_syn.out_table_create (
                schema                  => 'user_data'::regnamespace,
                in_table_id             => 'test_table',
                out_group_id            => 'out',
                out_columns             => ARRAY[
                       $COL$("sys_syn_trans_id_in","out_queue.trans_id_in",,,)$COL$,
                       $COL$("sys_syn_delta_type","out_queue.delta_type",,,)$COL$,
                       $COL$("sys_syn_queue_state","out_queue.queue_state",queue_state,"new.sys_syn_queue_state",)$COL$,
                       $COL$("sys_syn_queue_id","out_queue.queue_id",queue_id,"new.sys_syn_queue_id",)$COL$,
                       $COL$("sys_syn_queue_priority","out_queue.queue_priority",queue_priority,"new.sys_syn_queue_priority",)$COL$,
                       $COL$("sys_syn_hold_updated","out_queue.hold_updated",,,)$COL$,
                       $COL$("sys_syn_hold_trans_id_first","out_queue.hold_trans_id_first",,,)$COL$,
                       $COL$("sys_syn_hold_trans_id_last","out_queue.hold_trans_id_last",,,)$COL$,
                       $COL$("sys_syn_hold_reason_count","out_queue.hold_reason_count",,,)$COL$,
                       $COL$("sys_syn_hold_reason_id","out_queue.hold_reason_id",hold_reason_id,"new.sys_syn_hold_reason_id",)$COL$,
                       $COL$("sys_syn_hold_reason_text","out_queue.hold_reason_text",hold_reason_text,"new.sys_syn_hold_reason_text",)$COL$,
                       $COL$("sys_syn_trans_id_out","out_queue.trans_id_out",,,)$COL$,
                       $COL$("sys_syn_processed_time","out_queue.processed_time",processed_time,"new.sys_syn_processed_time",)$COL$,
                       $COL$("test_table_id","(out_queue.id).test_table_id",,,Id)$COL$,
                       $COL$("test_table_text","(in_source.attributes).test_table_text",,,Attribute)$COL$
                ]::sys_syn.create_out_column[],
                data_view               => 'false',
                out_log_lifetime        => NULL,
                out_partitions          => ARRAY[
                       $PART$()$PART$]::sys_syn.create_out_partition[],
                enable_adds             => 'true',
                enable_changes          => 'true',
                enable_deletes          => 'true',
                condition_sql           => NULL,
                records_per_claim       => '150000',
                claim_queue_count       => NULL,
                claim_fixed_by_id       => 'false',
                claim_random_sample     => NULL,
                queue_pid_used_age      => NULL,
                record_comparison_different=> NULL,
                record_comparison_same     => NULL
        );
----



===== Drop an Output Table

[source,sql]
----
SELECT sys_syn.out_table_drop('test_table', 'out');
----



==== Runtime Implementation



===== Pull Data

Pull the data from the source system using:

[source,sql]
----
SELECT user_data.test_table_pull(FALSE);
----

A boolean is returned.  False indicates that there are no records to process and that the following steps do not need to be run at this time.  True indicates that the following steps are ready to run.



===== Refresh the Output Queue

Refresh the changeset queue by calling the output group's move function:

[source,sql]
----
SELECT user_data.test_table_out_move_1();
----

A boolean is returned.  False indicates that there are no records to process and that the following steps do not need to be run at this time.  True indicates that the following steps are ready to run.

IMPORTANT:  The `move` function must be run in a transaction that is separate from the `pull` and `processed` functions.



===== Process the Output Queue

First, claim the `Unclaimed` records in the queue for processing by setting the `queue_state` to the `Claimed` status.

Run these 3 statements in the same transaction, or add BEGIN/COMMIT:

[source,sql]
----
BEGIN;

SELECT  sys_syn.in_trans_claim_start();

UPDATE  user_data.test_table_out_queue_1
SET     queue_state = 'Claimed'::sys_syn.queue_state
WHERE   queue_state = 'Unclaimed'::sys_syn.queue_state;

SELECT  sys_syn.in_trans_finish();

COMMIT;
----

Next, read only the records that have the `Claimed` status.

[source,sql]
----
SELECT  out_queue.*,
        (in_data.id).*,
        (in_data.attributes).*
FROM    user_data.test_table_out_queue_1 AS out_queue
        LEFT JOIN user_data.test_table_in_1 AS in_data USING (trans_id_in, id)
WHERE   queue_state = 'Claimed'::sys_syn.queue_state;
----

Process the records in your destination system.  For records that were processed successfully, set their `queue_state` to `Processed`.

If records failed to process, set their status to `Hold` or `Unclaimed`.  The `Hold` status allows you to process failed records at less frequent intervals.  The `Hold` status requires that you set `hold_reason_id` and/or `hold_reason_text`.

TIP:  If you update the `sys_syn` columns via the data_view, then you need to add `sys_syn_` in front of each `sys_syn` column's name.

[source,sql]
----
UPDATE  user_data.test_table_out_queue_1 AS out_queue
SET     queue_state = 'Processed'::sys_syn.queue_state
WHERE   (out_queue.id).test_table_id = 1;

UPDATE  user_data.test_table_out_queue_1 AS out_queue
SET     queue_state = 'Hold'::sys_syn.queue_state,
        hold_reason_text = 'This object has been put on hold for an example.'
WHERE   (out_queue.id).test_table_id = 2;
----



===== Commit the Processing Statuses

Updating the `queue_state` does not automatically commit the processing status.  Call the output's `processed` function to commit the processed changes.  This removes processed records from the queue table and commits them into baseline status so that future changesets only contain actual changes.

[source,sql]
----
SELECT user_data.test_table_out_processed_1();
----

A boolean is returned.  False indicates that there was nothing to do.  True indicates that the queue state was changed.



===== Error Handling

If you use the `Hold` status, then you must set the `Hold` status back to `Unclaimed` when you want to retry those records.  The `hold_reason_count` value is incremented if the error is the same error that was recorded in the prior processing attempt.  This allows you to implement a backoff algorithm to avoid wasting resources on a potentially non-transient failure.

If the object changes value while in the `Hold` status, then its queue status is automatically reset to `Unclaimed`.  This allows data corrections to be retried without a `Hold` delay.



==== Advanced

===== in_table_transforms

When a new table is added, the rules in the `sys_syn.in_table_transforms` table sets arguments to the `sys_syn.in_table_create` function when generating the call from `sys_syn.in_table_create_sql`.  The rule is applied when all criteria that is specified in the rule are true.

.Columns
rule_group_id::
    NULL for a rule that applies to all tables.
priority::
    The order that the rule is applied.
relation_name_like::
    The rule is applied to the table when the relation name matches this `LIKE` pattern.
in_group_id_like::
    The rule is applied to the table when the in_group_id matches this `LIKE` pattern.
schema_like::
    The rule is applied to the table when the schema matches this `LIKE` pattern.
in_table_id_like::
    The rule is applied to the table when the in_table_id matches this `LIKE` pattern.
in_pull_id_like::
    The rule is applied to the table when the in_pull_id matches this `LIKE` pattern.
enable_deletes_implied::
    The rule is applied to the table when enable_deletes_implied is this value.
null_key_handler::
    The rule is applied to the table when null_key_handler is this value.
key_violation_handler::
    The rule is applied to the table when key_violation_handler is this value.
full_prepull_id_like::
    The rule is applied to the table when the full_prepull_id matches this `LIKE` pattern.
changes_prepull_id_like::
    The rule is applied to the table when the changes_prepull_id_like matches this `LIKE` pattern.
new_in_table_id::
    Change the tables's new_in_table_id to this.
new_in_pull_id::
    Change the tables's new_in_pull_id to this.
new_full_sql::
    Change the tables's new_full_sql to this.
new_changes_sql::
    Change the tables's new_changes_sql to this.
new_full_pre_sql::
    Change the tables's new_full_pre_sql to this.
new_changes_pre_sql::
    Change the tables's new_changes_pre_sql to this.
new_full_post_sql::
    Change the tables's new_full_post_sql to this.
new_changes_post_sql::
    Change the tables's new_changes_post_sql to this.
new_enable_deletes_implied::
    Change the tables's new_enable_deletes_implied to this.
new_null_key_handler::
    Change the tables's new_null_key_handler to this.
new_key_violation_handler::
    Change the tables's new_key_violation_handler to this.
new_full_prepull_id::
    Change the tables's new_full_prepull_id to this.
new_changes_prepull_id::
    Change the tables's new_changes_prepull_id to this.
new_record_comparison_different::
    Change the tables's new_record_comparison_different to this.
new_record_comparison_same::
    Change the tables's new_record_comparison_same to this.
new_in_partition::
    Change the tables's new_in_partition to this.
new_in_partition_count::
    Change the tables's new_in_partition_count to this.
new_in_partitions::
    Change the tables's new_in_partitions to this.
omit::
    Omit this table.
final_ids::
    Stop processing rules with any of these IDs.
final_rule::
    Stop processing all rules after this one.
comments::
    If you want to add comments about this rule in this table, add them in this column.



===== in_column_transforms

When new tables are added, the rules in the `sys_syn.in_column_transforms` table adds, modifies, or removes columns.  The rule is applied when all criteria that is specified in the rule are true.

.Columns
rule_group_id::
    NULL for a rule that applies to all tables.
priority::
    The order that the rule is applied.
data_type_like::
    The rule is applied to the column when the data type matches this `LIKE` pattern.
relation_name_like::
    The rule is applied to the column when the relation_name_like matches this `LIKE` pattern.
in_column_type::
    The rule is applied to the column when the in_column_type is this value.
column_name_like::
    The rule is applied to the column when the column name matches this `LIKE` pattern.
in_table_id_like::
    The rule is applied to the column when the in_table_id matches this `LIKE` pattern.
in_group_id_like::
    The rule is applied to the column when the in_group_id matches this `LIKE` pattern.
in_pull_id_like::
    The rule is applied to the column when the in_pull_id matches this `LIKE` pattern.
schema_like::
    The rule is applied to the column when the schema matches this `LIKE` pattern.
is_key::
    The rule is applied to the column when the column's primary or foreign status is this value.
primary_in_table_id_like::
    The rule is applied to the column when the foreign or primary key points to an `in_table_id` that matches this `LIKE` pattern.  The primary_column_name_like column is required when this is used.
primary_column_name_like::
    The rule is applied to the column when the foreign or primary key points to a `column_name` that matches this `LIKE` pattern.  The primary_in_table_id_like column is required when this is used.
new_data_type::
    Change the column's data type to this.
new_in_column_type::
    Change the column's in_column_type to this.
new_column_name::
    Change the column's name to this.
new_array_order::
    Change the column's array_order to this.
expression::
    Specify an expression for this column.  The prior column or expression can be referenced by %1
create_in_columns::
    Add the specified columns.
omit::
    Omit this column from the table.  If a variable_name was specified, the associated expression is stored into this variable.  This can be accessed from other expressions.
final_ids::
    Stop processing rules with any of these IDs.
final_rule::
    Stop processing all rules after this one.
comments::
    If you want to add comments about this rule in this table, add them in this column.



===== foreign_keys_for_c_sql

If you use `sys_syn.in_table_create_sql` to create the call to `in_table_create`, then you may want to use the `sys_syn.foreign_keys_for_c_sql` table to define foreign keys.  When you do this, `in_table_create_sql` will insert these foreign keys into the `in_columns` array when they are referenced.

The following is an example of a value insert, but it is more practical to export all of the foreign keys from your source database and import that into this table.  Use the `database_path` identifier to delete all of the keys related to a specific database when you need to refresh that database's foreign key data again.

[source,sql]
----
DELETE FROM sys_syn.foreign_keys_for_c_sql
WHERE   database_path = 'server_name/service_instance/database_name';

INSERT INTO sys_syn.foreign_keys_for_c_sql (
        database_path,                                  foreign_key_id,
        foreign_in_table_id,                            primary_in_table_id,
        foreign_column_name,                            primary_column_name)
VALUES ('server_name/service_instance/database_name',   'test_fkey',
        'child_table',                                  'parent_table',
        'parent_table_id',                              'parent_table_id');
----



=== More Examples

See the `test` directory for more examples.



=== Supplemental Information


==== Server Performance Tuning

At http://pgtune.leopard.in.ua/, set the `DB Type` to `Data warehouses`.  Enter the memory available to PostgreSQL.  Subtract the memory usage for the other services that run on the same machine.  For `Number of Connections`, account for the `superuser_reserved_connections` value.  If `sys_syn` and `sys_syn_dblink` run on the same server, double the number of connections to account for the dblink connections.  Make the recommended changes.

Increase the `max_locks_per_transaction` value depending on the number of tables that you add and how many partitions that they use.

Adjust the `bgwriter` values with http://blog.postgresql-consulting.com/2017/03/deep-dive-into-postgres-stats_27.html.



==== Testing Performance Enhancement

Optionally, you can initialize a database server in shared memory to avoid disk I/O.  This useful if you need to run the tests frequently.



===== Initialize In Memory Server

[source,shell]
----
export PGDATA=/dev/shm/$USER-pg_regression_test
mkdir "$PGDATA"
initdb --auth-local=peer --auth-host=ident -U postgres -N "$PGDATA"
cat << "EOF" >> "$PGDATA/postgresql.conf"
fsync = off
synchronous_commit = off
full_page_writes = off
random_page_cost = 1.0
update_process_title = off
EOF
echo "CREATE ROLE $USER SUPERUSER CREATEDB CREATEROLE INHERIT LOGIN" | postmaster --single -D "$PGDATA" -F -h "" -k "$PGDATA" postgres && echo
postmaster -D "$PGDATA" -F -h "" -k "$PGDATA" & sleep 2; echo
export PGHOST=$PGDATA
----

CAUTION:  Every program launched in this terminal will point to this in memory instance.  Be careful not to accidentally put non-ephemeral data or code there.

CAUTION:  Remember that everything created in this database will disappear after a reboot, shutdown, or machine crash.

TIP:  To view this instance in pgAdmin3, set the +Host+ to +/dev/shm/$USER-pg_regression_test+, replace +$USER+ with your user name (run +echo $USER+ if you do not know what it is), and leave the +Port+ number as +5432+.  Use the same user name for the +Username+ field.  When prompted for a password, leave it blank or enter any non-blank value to save it.



===== Run Tests

[source,shell]
----
sudo PATH=$PATH make clean && sudo PATH=$PATH make install && make installcheck
----



===== Shutdown & Delete In Memory Server

The following commands will shutdown the server and permanently delete all of the data that was created within that server.

[source,shell]
----
fg 1
----

Hold Ctrl and press C.

[source,shell]
----
rm -Rf "/dev/shm/$USER-pg_regression_test"
unset PGDATA
unset PGHOST
----


==== PL/pgSQL Debugger

You can use the PL/pgSQL debugger in pgAdmin3 if you build and install the following extension.  You may want to change the install directory and use a different server restart command depending on your distribution and instance.  If you have access, the `/usr/local/src` directory is a good location to store the source code.  However, you will not need it again.  If you upgrade PostgreSQL to a different major version, you will need to download a fresh copy and install it again.

CAUTION:  If you already have something in `shared_preload_libraries`, then manually edit `$PGDATA/postgresql.conf` and add `$libdir/plugin_debugger` to `shared_preload_libraries` instead of running the `cat` command below.

[source,shell]
----
cd /dev/shm
curl -LO "http://ftp.postgresql.org/pub/source/v$(pg_config --version | cut -f 2 -d ' ')/postgresql-$(pg_config --version | cut -f 2 -d ' ').tar.bz2"
tar -xjf postgresql-$(pg_config --version | cut -f 2 -d ' ').tar.bz2
cd postgresql-$(pg_config --version | cut -f 2 -d ' ')
USE_PGXS=1 ./configure
USE_PGXS=1 make
cd contrib
git clone "git://git.postgresql.org/git/pldebugger.git"
make
cd pldebugger
USE_PGXS=1 make
sudo USE_PGXS=1 PATH=$PATH make install

cat << "EOF" >> "$PGDATA/postgresql.conf"
shared_preload_libraries = '$libdir/plugin_debugger'
EOF

pg_ctl restart
----

After the restart, you need to add the `pldbgapi` extension on each database that you want to use the debugger with.

[source,sql]
----
CREATE EXTENSION pldbgapi;
----



==== Compressed Tablespace

Synchronization requires about 3 times the storage requirements of the uncompressed source data (assuming that you sync every row and column), depending on the primary key length relative to the other columns.  Using compression may allow the synchronization process to only consume the same amount of storage as the source system.  Tables that use a single 32-bit integer for their primary key require less storage.  Using a compressed file system can be very beneficial for both performance and storage efficiency.  The following lets you evaluate a compressed tablespace.  Do not use these file image based file systems for any purpose other than this evaluation.  Because these commands affect the file systems on the machine, they are best run on a machine that is easily reinstalled, such as a new virtual machine that is dedicated for this evaluation.

Change the file size from 3g to your desired file size.  A 3GB file will store about 9GB of synchronization data.  This will store about 3GB of data from your source system.  If `fallocate` fails because it is not supported for your file system, try "`dd if=/dev/zero of=/opt/var-lib-pgsql-compressed.img bs=1G count=3`"  If you are on a Btrfs file system, run "`touch /opt/var-lib-pgsql-compressed.img`" and "`chattr +C /opt/var-lib-pgsql-compressed.img`" before running `fallocate`.

Btrfs may still have some recovery issues when using compression, so ZFS is a better option at this time.



===== Btrfs on EL 7.x

The following commands create a proof of concept.  For a more permanent solution, use a more typical Btrfs setup.  For that, do not use `single` metadata when creating the file system and do not turn off `checksums` when mounting it.  Some backup software require that instead of turning `atime` off, you leave the default `relatime` on.

WARNING:  You will need to start PostgreSQL manually after rebooting if you follow these instructions.

IMPORTANT:  You may need to change `loop0` to something else if that name is already in use.  Run "`ls /dev/loop0`" to check for its existence.  If you need to change it, also change it in the "after reboot" instructions below.

[source,shell]
----
sudo fallocate -l 3g /opt/var-lib-pgsql-compressed.img
sudo losetup /dev/loop0 /opt/var-lib-pgsql-compressed.img
sudo mkfs.btrfs -m single /dev/loop0
sudo mkdir /var/lib/pgsql/compressed
sudo mount -o compress-force=lzo,noatime,nodatasum /dev/loop0 /var/lib/pgsql/compressed
sudo chcon system_u:object_r:postgresql_db_t:s0 /var/lib/pgsql/compressed
sudo chown postgres:postgres /var/lib/pgsql/compressed
sudo chmod 700 /var/lib/pgsql/compressed
sudo systemctl disable postgresql-9.6.service
----

The last line prevents PostgreSQL from starting on boot because you will need to mount the additional tablespace manually before PostgreSQL can start.

Create a tablespace and create a database inside of it:

[source,sql]
----
CREATE TABLESPACE compressed LOCATION '/var/lib/pgsql/compressed';
CREATE DATABASE sys_syn_compressed WITH TABLESPACE=compressed;
----

When you reboot, you will need to run the following commands before PostgreSQL can access the tablespace:

[source,shell]
----
sudo losetup /dev/loop0 /opt/var-lib-pgsql-compressed.img
sudo mount -o compress-force=lzo,noatime,nodatasum /dev/loop0 /var/lib/pgsql/compressed
sudo systemctl start postgresql-9.6.service
----

You can view the sizes with:

[source,shell]
----
sudo btrfs filesystem usage /var/lib/pgsql/compressed
----

TIP:  Although the `nodatacow` option is useful for databases, it disables compression.



===== ZFS on EL 6.x or 7.x

The following commands create a proof of concept.  For a more permanent solution, create a typical ZFS pool.  For that, do not use `redundant_metadata=most`.  Also consider using multiple datasets per pool if you have a need for snapshots that only need to operate on a dataset within a pool.  Do not turn off checksums if the pool has redundancy.  If the ZFS pool has no redundancy and you want to use checksums, enable them in PostgreSQL.  PostgreSQL checksums verifies the data's integrity through more layers than storage checksums.  However, storage checksums are essential for utilizing the storage redundancy features.  Some backup software require that instead of turning `atime` off, you turn `relatime` on.  If you create separate pools or datasets for the WAL, you do not need "`logbias=throughput`" for the WAL.  "`logbias=throughput`" is beneficial for the data.

WARNING:  This requires Dynamic Kernel Module Support (DKMS).  This will build kernel modules from source code for each kernel version that you run.

[source,shell]
----
sudo yum install kernel-devel-$(uname -r)
----

If you receive the following error, then you will need to perform a "`sudo yum update`".  After running that, reboot ("`sudo shutdown -r now`") and run the above line again.

`No package kernel-devel-VERSION available.`

Once the kernel-devel package that matches the running kernel is installed, continue with:

TIP:  If you already have the Extra Packages for Enterprise Linux (EPEL) repository installed, then you can skip the first line.

[source,shell]
----
sudo yum install epel-release
sudo yum install "http://download.zfsonlinux.org/epel/zfs-release.$(uname -r | egrep -o 'el+[0-9]+').noarch.rpm"
sudo yum install zfs
sudo modprobe zfs
sudo fallocate -l 3g /opt/var-lib-pgsql-compressed.img
sudo zpool create var-lib-pgsql-compressed /opt/var-lib-pgsql-compressed.img
sudo zfs set mountpoint=/var/lib/pgsql/compressed var-lib-pgsql-compressed
sudo chcon system_u:object_r:postgresql_db_t:s0 /var/lib/pgsql/compressed
sudo chown postgres:postgres /var/lib/pgsql/compressed
sudo chmod 700 /var/lib/pgsql/compressed
sudo zfs set rootcontext=system_u:object_r:postgresql_db_t:s0 var-lib-pgsql-compressed
sudo zfs set fscontext=system_u:object_r:postgresql_db_t:s0 var-lib-pgsql-compressed
sudo zfs set context=system_u:object_r:postgresql_db_t:s0 var-lib-pgsql-compressed
sudo zfs set defcontext=system_u:object_r:postgresql_db_t:s0 var-lib-pgsql-compressed
sudo zfs set recordsize=8K var-lib-pgsql-compressed
sudo zfs set compression=lz4 var-lib-pgsql-compressed
sudo zfs set redundant_metadata=most var-lib-pgsql-compressed
sudo zfs set primarycache=metadata var-lib-pgsql-compressed
sudo zfs set secondarycache=metadata var-lib-pgsql-compressed
sudo zfs set logbias=throughput var-lib-pgsql-compressed
sudo zfs set dedup=off var-lib-pgsql-compressed
sudo zfs set checksum=off var-lib-pgsql-compressed
sudo zfs set atime=off var-lib-pgsql-compressed > /dev/null 2>&1
----

For EL 6 only:

[source,shell]
----
sudo chkconfig zfs-import on
sudo chkconfig zfs-mount on
sudo chkconfig zfs-share on
sudo chkconfig zfs-zed on
----

For EL 7 only:

[source,shell]
----
sudo systemctl enable zfs-import-cache.service
sudo systemctl enable zfs-import-scan.service
sudo systemctl enable zfs-mount.service
sudo systemctl enable zfs-share.service
sudo systemctl enable zfs-zed.service
sudo systemctl enable zfs.target
----

Create a tablespace and create a database inside of it:

[source,sql]
----
CREATE TABLESPACE compressed LOCATION '/var/lib/pgsql/compressed';
CREATE DATABASE sys_syn_compressed WITH TABLESPACE=compressed;
----

You can view the compressed size, compression ratio, and uncompressed size with:

[source,shell]
----
sudo df -h /var/lib/pgsql/compressed
sudo zfs get compressratio var-lib-pgsql-compressed
sudo du -h -s --apparent-size /var/lib/pgsql/compressed
----



== Copyright and License

Copyright (c) 2016-2017.

Legal Notice:  See the COPYRIGHT file.

`sys_syn` copyright is novated to PostgreSQL Global Development Group.
